{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_slrqi3NNPc",
        "outputId": "9ca15231-9a22-4c5a-c392-83a9ba75cc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[[ -2.22  -2.22  72.85  -2.66]\n",
            " [ 66.06  77.74  70.56  -2.59]\n",
            " [ 77.1   72.8   82.88  70.71]\n",
            " [ 50.39  -1.58  61.34  77.74]\n",
            " [ 68.04  77.74  53.71  72.24]\n",
            " [ 66.11  82.88  57.59  49.79]\n",
            " [ 77.74  88.3   88.28  77.73]\n",
            " [ 72.85  85.88  94.    82.88]\n",
            " [ -2.12  82.88  -2.05  68.33]\n",
            " [ 75.6   88.3   80.27  54.  ]\n",
            " [ 82.88  94.    65.59  82.88]\n",
            " [ 88.3   93.99 100.    88.29]\n",
            " [ 47.31  88.3   -1.38  -1.38]\n",
            " [ 57.75  94.    80.28  82.21]\n",
            " [ 85.9  100.    93.77  80.35]\n",
            " [  0.     0.     0.     0.  ]]\n",
            "\n",
            "Optimal Policy:\n",
            "State 0: ↓\n",
            "State 1: →\n",
            "State 2: ↓\n",
            "State 3: ←\n",
            "State 4: →\n",
            "State 5: →\n",
            "State 6: →\n",
            "State 7: ↓\n",
            "State 8: →\n",
            "State 9: →\n",
            "State 10: →\n",
            "State 11: ↓\n",
            "State 12: →\n",
            "State 13: →\n",
            "State 14: →\n",
            "State 15: ↑\n",
            "\n",
            "Optimal Path from State 0 to Goal:\n",
            "0 -> 4 -> 5 -> 6 -> 7 -> 11 -> 15\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the environment (4x4 grid)\n",
        "n_states = 16  # states: 0 to 15\n",
        "actions = [0, 1, 2, 3]  # up, right, down, left\n",
        "\n",
        "# Rewards dataset: -1 for each move, 0 for goal\n",
        "rewards = np.full(n_states, -1)\n",
        "goal_state = 15\n",
        "rewards[goal_state] = 100\n",
        "\n",
        "# Define possible transitions (walls at edges)\n",
        "def next_state(state, action):\n",
        "    row, col = divmod(state, 4)\n",
        "    if action == 0 and row > 0:\n",
        "        row -= 1  # up\n",
        "    elif action == 1 and col < 3:\n",
        "        col += 1  # right\n",
        "    elif action == 2 and row < 3:\n",
        "        row += 1  # down\n",
        "    elif action == 3 and col > 0:\n",
        "        col -= 1  # left\n",
        "    return row * 4 + col\n",
        "\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((n_states, len(actions)))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.7  # learning rate\n",
        "gamma = 0.95  # discount factor\n",
        "epsilon = 0.1  # exploration probability\n",
        "episodes = 500\n",
        "\n",
        "# Q-Learning algorithm\n",
        "for episode in range(episodes):\n",
        "    state = random.randint(0, n_states - 1)\n",
        "\n",
        "    while state != goal_state:\n",
        "        # ε-greedy action selection\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state, :])\n",
        "\n",
        "        new_state = next_state(state, action)\n",
        "        reward = rewards[new_state]\n",
        "\n",
        "        # Update Q-value\n",
        "        Q[state, action] = Q[state, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[new_state, :]) - Q[state, action]\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "# Show final Q-table\n",
        "print(\"Q-Table:\")\n",
        "print(np.round(Q, 2))\n",
        "\n",
        "# Extract optimal policy\n",
        "optimal_policy = np.argmax(Q, axis=1)\n",
        "action_map = ['↑', '→', '↓', '←']\n",
        "\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for i in range(n_states):\n",
        "    print(f\"State {i}: {action_map[optimal_policy[i]]}\")\n",
        "\n",
        "# Demonstrate a path from start (state 0)\n",
        "state = 0\n",
        "path = [state]\n",
        "while state != goal_state:\n",
        "    action = optimal_policy[state]\n",
        "    state = next_state(state, action)\n",
        "    path.append(state)\n",
        "\n",
        "print(\"\\nOptimal Path from State 0 to Goal:\")\n",
        "print(\" -> \".join(map(str, path)))"
      ]
    }
  ]
}